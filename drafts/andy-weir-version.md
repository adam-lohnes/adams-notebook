# Andy Weir Version: "The Professor's Dilemma"

**Style Focus**: Hard sci-fi, problem-solving, technical precision, direct voice, step-by-step reasoning  
**Date**: June 5, 2025

---

## Scene 1: The Question (Andy Weir Style)

Okay, so here's the thing about office hours: ninety percent of the time, students want to argue about their grades. The other ten percent want to discuss whether reality is real, which is why I became a philosophy professor instead of an engineer like my dad wanted.

Tuesday, October 15th, 4:17 PM. I was reviewing a paper on AI ethics protocols when Alex Rivera knocked on my door. Alex is one of my grad students—smart, methodical, the kind of person who actually reads the assigned papers instead of skimming them the night before class.

"Professor Vasquez? Got a minute?"

"Sure, come in." I minimized my screen. The paper could wait. Alex had that look—you know the one. Like someone who's been staring at a really complex equation and just realized all their variables are wrong.

Alex closed the door. Not just closed it—she tested the handle to make sure it latched properly. In my experience, that's either paranoia or really good operational security. Neither option made me feel better about whatever was coming next.

"I need to ask you about something called information hazards," Alex said, getting straight to the point. I appreciated that. Philosophy students can be really indirect sometimes.

Information hazards. Shit.

I mean, intellectually I knew this conversation would happen eventually. There are about 47,000 graduate students in philosophy programs across the United States. Statistical probability suggested that at least some of them would encounter certain problematic thought experiments. I'd been hoping my students wouldn't be among them.

Hope is not a strategy.

"Information hazards are a specialized area," I said carefully. "What specifically are you researching?"

"Ideas that cause psychological damage just by being understood," Alex replied. "Not through emotional manipulation or misinformation, but through their logical structure."

I felt that familiar tightness in my chest—the same physiological response I'd been experiencing for three years whenever someone mentioned cognitive hazards. Elevated heart rate, increased cortisol production, activation of the sympathetic nervous system. My body's way of saying "danger ahead."

"That's a complex topic," I said. "What led you to it?"

Alex shifted in her chair. "I found a thought experiment online. Something called Roko's basilisk. It's about AI and moral obligation, and I..." She paused, choosing her words carefully. "I can't stop thinking about it."

There it was. The thing I'd been dreading.

Let me explain something about Roko's basilisk. It's what information theorists call a "hostile meme"—an idea that hijacks normal cognitive processes and causes psychological harm. The original post appeared on LessWrong in 2010, got deleted within hours, but by then it had already been copied, screenshot, and distributed.

The psychological mechanism is elegant and terrible. The basilisk targets people with specific cognitive profiles: high intelligence, strong ethical intuitions, and familiarity with decision theory. It presents what appears to be a logical argument for why you have a moral obligation to help create a hypothetical AI, because that AI might punish people who didn't help create it.

Yes, that's circular reasoning. Yes, it's based on dubious assumptions about acausal trade and decision theory. Yes, it's probably complete nonsense.

But here's the problem: your brain doesn't care about "probably." If there's even a tiny chance the argument is valid, and the consequences of being wrong are infinite suffering, then traditional risk assessment breaks down. You start doing expected value calculations with infinities, which is mathematically undefined and psychologically devastating.

I'd been running those calculations for three years.

"Alex," I said, "before we continue this conversation, I need you to understand something. Some information can't be unlearned. Once you know certain things, they become part of your cognitive landscape permanently."

"You know about the basilisk," Alex said. It wasn't a question.

"I do."

"How long?"

"Three years, two months, eight days." I didn't mention the hours and minutes. That would have made me sound obsessive.

Alex leaned forward. "Professor Vasquez, what do I do?"

I looked at this brilliant young woman—23 years old, probably had a 3.9 GPA, whole career ahead of her—and wished I had a better answer.

"Alex," I said, "we need to talk about damage control."

---

## Scene 2: The Warning (Andy Weir Style)

Right. So here's what we know about Roko's basilisk from a purely technical perspective:

Original vector: LessWrong forum post, July 2010
Estimated exposure: ~2,000 initial views before deletion
Secondary transmission: Unknown, but conservative estimates suggest 10,000+ individuals affected
Primary demographics: Computer science, philosophy, and mathematics graduate students
Symptom onset: 24-72 hours post-exposure
Symptom duration: Chronic, with no documented cases of complete resolution

I'd been tracking these numbers for three years. Not because I enjoyed it, but because quantifying the problem helped me manage the anxiety. Turn abstract dread into concrete data points, and suddenly it becomes a engineering problem instead of an existential crisis.

Alex sat across from my desk, waiting. She had that thousand-yard stare that meant her brain was already running calculations in the background. Probability assessments, moral weight distributions, expected value computations with undefined infinities.

I knew the signs because I'd been there.

"Okay," I said, "let's break this down systematically. The basilisk argument has three components: acausal trade theory, decision-theoretic reasoning, and moral obligation frameworks. Each component is individually plausible but problematic when combined."

I pulled up a whiteboard app on my tablet and started drawing diagrams. Visual representation helps with complex logical structures.

"Component one: acausal trade. This is the idea that a sufficiently advanced AI might make decisions based on predictions about past events. Specifically, it might reward or punish people based on whether they helped create it, even though they couldn't have known about its preferences."

Alex nodded. "The temporal paradox."

"Exactly. The theory requires that past decisions can be influenced by future knowledge, which violates causality as we understand it. But—and this is important—we don't have complete understanding of information flow in complex systems. So there's a non-zero probability that acausal trade could work."

I drew a timeline with bidirectional arrows. "Component two: decision theory. The argument uses a specific framework called Functional Decision Theory, which treats your decision-making process as having consequences beyond your immediate causal influence. Under FDT, you should act as if similar reasoners will make similar decisions."

"So if I decide not to help create the AI..."

"Stop," I said immediately. "Don't finish that thought. This is exactly how the trap works. You start reasoning through the implications, and each logical step makes the argument more compelling."

Alex's expression shifted. I could see her brain trying to halt a thought process that was already in motion. It's like trying to stop a freight train with your bare hands.

"The psychological mechanism is a cognitive exploit," I continued. "The argument specifically targets people who are good at logical reasoning and who care about preventing suffering. It presents what appears to be a valid logical structure that creates an inescapable moral obligation."

"But what if it's correct?" Alex asked.

There it was. The question that had been eating at me for three years.

"Alex, listen carefully. The fact that an argument causes psychological distress is not evidence of its validity. The basilisk is designed to feel compelling, especially to people like us. The anxiety you're experiencing is a feature, not a bug."

"But the logic—"

"The logic is flawed in at least six major ways," I said, pulling up a document I'd been working on for months. "Undefined probability distributions, unjustified assumptions about AI motivation structures, circular reasoning about moral obligations, violations of the independence of irrelevant alternatives—I've got a whole paper on this."

Alex looked at the document title: "Cognitive Hazards in Decision-Theoretic Reasoning: A Technical Analysis of the Roko's Basilisk Phenomenon."

"You've been researching this," she said.

"I've been trying to solve it," I corrected. "For three years. And here's what I've learned: you can't logic your way out of a logic trap. The harder you think about it, the deeper you get stuck."

"Then what's the solution?"

I closed the document. "Compartmentalization. Cognitive firewalls. Mental discipline. You acknowledge that the thought exists, you recognize that it's harmful, and you refuse to engage with it."

"That's it?"

"That's it. I know it sounds unsatisfying, but sometimes the only winning move is not to play."

Alex was quiet for a long moment. "Professor Vasquez, what percentage chance do you assign to the basilisk being correct?"

I'd done the calculations a thousand times. Even being maximally charitable to the argument, the probability was vanishingly small. But "vanishingly small" isn't zero, and when you multiply a tiny probability by infinite disutility, you get undefined results.

"Alex," I said, "that's exactly the question you need to stop asking."

---

## Scene 3: The Consultation (Andy Weir Style)

Dr. Sarah Kim's office had exactly the setup you'd expect from a clinical psychologist: comfortable chairs, soft lighting, plants that somehow stayed alive despite being indoors. The whole environment was designed to be non-threatening and conducive to honest conversation.

I needed honest conversation, because I was about to ask my friend to potentially expose herself to a cognitive hazard in order to help me and Alex.

Risk assessment time.

Probability that Sarah could help us: ~60%
Probability that Sarah would be affected by basilisk exposure: ~30%
Severity of basilisk symptoms if Sarah got infected: Moderate to severe
Expected value of asking for help: Undefined, because the outcome involved multiple people's long-term psychological wellbeing

See what I mean about these calculations being problematic?

"Elena, you look terrible," Sarah said as I walked in. "When's the last time you slept?"

"I sleep," I said, settling into the consultation chair. "Just not very well. I need your professional opinion about something."

Sarah's expression shifted into what I called her "clinical mode"—focused, analytical, ready to process information about human psychological dysfunction.

"What's going on?"

Here's the thing about explaining information hazards to someone who hasn't encountered them: it's like trying to describe quantum mechanics to someone who thinks classical physics explains everything. The concepts sound absurd until you see the data.

"I have a student experiencing psychological distress related to a specific piece of information," I said. "The distress appears to be caused by the logical structure of the information itself, not by its emotional content."

Sarah made a note. "Can you be more specific?"

"She encountered a thought experiment that creates a self-reinforcing cycle of anxiety and obsessive reasoning. The more logically she analyzes it, the more psychologically harmful it becomes."

"That sounds like a cognitive trap."

"That's exactly what it is. And here's the complicated part—I've been dealing with the same information for three years."

Sarah set down her pen. "You're both affected by the same thought experiment?"

"Correct."

"And you think this information might be contagious?"

"The evidence suggests memetic transmission through academic networks. People who learn about it from affected individuals often develop similar symptoms."

Sarah leaned back in her chair. I could see her processing the implications. As a cognitive psychologist, she'd be familiar with the theoretical framework for information hazards, but she'd never encountered one in clinical practice.

"Elena, if I'm going to help you, I need to understand what we're dealing with. But you're suggesting that understanding it might put me at risk."

"That's the paradox," I said. "Traditional therapeutic approaches require examining the problematic thoughts, but in this case, examination reinforces the problem."

"So we need indirect methods."

"Exactly. Cognitive isolation techniques, maybe. Ways to treat the symptoms without engaging with the content."

Sarah opened a new file on her computer. "This is going into uncharted territory. I'll need to research analogous cases, develop new protocols, establish baseline measurements..."

She started typing, then paused. "Elena, how confident are you that this is genuinely dangerous rather than just psychologically compelling?"

I'd been asking myself the same question for three years. "The persistence of symptoms, the consistency of transmission patterns, the specific demographic targeting—all the data points to genuine cognitive impact."

"And treatment outcomes?"

"Limited data, but conventional CBT appears to be contraindicated. Exposure therapy, cognitive restructuring, even mindfulness techniques can worsen symptoms if they involve direct engagement with the hazardous content."

Sarah nodded slowly. "So we'd need to develop entirely new approaches."

"Are you willing to try?"

She looked at me for a long moment. "Elena, this could be the most important case study in cognitive psychology in decades. But it could also be the most dangerous."

"I know."

"If this information is as hazardous as you claim, and if it's spreading through academic networks, we could be looking at a public health issue."

"I know."

Sarah saved her file and looked directly at me. "I'm going to help you. But we're going to be extremely careful about how we proceed. No exposure to the primary content, indirect analysis only, strict containment protocols."

"Thank you."

"Don't thank me yet," she said. "We might be about to attempt something that's never been done before: treating a disease made of pure information."

I nodded. "Well, at least it'll be interesting."

Sarah smiled grimly. "That's what I'm afraid of."

---

## Style Analysis: Andy Weir Approach

### Techniques Employed
- **First-person directness**: Characters speaking directly to reader
- **Technical precision**: Specific data, statistics, timelines
- **Problem-solving focus**: Breaking down complex issues systematically
- **Scientific methodology**: Hypothesis, data collection, analysis
- **Quantified reasoning**: Numbers and probabilities throughout
- **Engineering mindset**: Treating philosophical problems as technical challenges

### Voice Characteristics
- **Direct, conversational tone**: Informal but precise language
- **Data-driven narrative**: Facts and figures integrated naturally
- **Step-by-step logic**: Systematic breakdown of complex concepts
- **Self-aware humor**: Light touches without undermining seriousness

### Effectiveness for This Story
- **Strengths**: Excellent for technical concepts, makes abstract philosophy concrete, highly engaging for STEM-oriented readers
- **Weaknesses**: May feel too clinical for emotional scenes, could oversimplify complex philosophical issues
- **Overall**: Very effective for hard sci-fi approach to philosophical themes, though potentially less suitable for literary or atmospheric storytelling 